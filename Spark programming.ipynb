{"cells":[{"cell_type":"code","source":["# Creating RDDs \n# RDDs can be created from in-memory collections, or from data stored in external data sources (e.g., files on HDFS).  \n# The example below creates an RDD from an in-memory collection:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":true,"inputWidgets":{},"nuid":"c32b1ca5-6480-4f58-95e9-aea1b68e97cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# RDD(Resilient Distributed Dataset)\nwordsRDD=sc.parallelize([\"fish\",\"cat\",\"dog\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25f668bc-5f4a-44aa-9279-9e1b06dc1a75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["wordsRDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2424b569-cc22-4312-81e3-2619e9cbbfb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[173]: ParallelCollectionRDD[230] at readRDDFromInputStream at PythonRDD.scala:413</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[173]: ParallelCollectionRDD[230] at readRDDFromInputStream at PythonRDD.scala:413</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Provide an instance of an iterable object (sequence)\n# as argument. Note that the Spark Context is automatically \n# instantiated by the notebook, and is bound to the 'sc' variable.\n# It will need to be created explicitly for a standalone program.\nrdd = sc.parallelize(range(30))\nrdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79ec3f19-4712-462a-92ef-40df032cc98c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[175]: PythonRDD[234] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[175]: PythonRDD[234] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Spark Context can also be accessed through the Spark Session object, which is\n# automatically created and bound to the variable 'spark'\nrdd1 = spark.sparkContext.parallelize(range(3, 5))\nrdd1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8531faf2-554f-41c0-b1a5-6b3cc9599943"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[176]: PythonRDD[236] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[176]: PythonRDD[236] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Collect\n# An RDD does not live in the local memory of a driver program (unless in local mode), but rather is broken into pieces and distributed across a cluster. While there, it undergoes a series of transformations as prescribed by the driver program (which is converted into a lineage graph, or DAG). The transformations are not actually executed until an action is called. An action causes data to be produced and materialized in either the driver program memory, or an external store (e.g., HDFS).\n\n# The collect() action below causes the RDD content to be shipped back to a driver program and materialized in its memory as a list. Do not call it unless you are sure your RDD is sufficiently small!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e80749ac-3435-4829-9323-5627870ff0f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["result = rdd.collect()\nprint(type(result))\nprint(result)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01df9b41-1a00-456d-867a-94f861ba32be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["result = rdd1.collect()\nprint(type(result))\nprint(result)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"397179d5-65c4-42a9-94b3-8f8b050c3373"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n[3, 4]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n[3, 4]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Map\n# Map transformation is one of the most commonly used. As in Python, it applies a given function to every element of an RDD, and returns the resulting RDD. Note that this new RDD is not actual data! It is just an abstract handle encapsulating the transformation outcome that we can use to invoke another transformation or action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb0f256c-6217-4178-a9f2-8585b1c7e488"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(rdd.map(lambda x: x*x).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b8c2826-38a5-4e3d-b42f-e3a36a8b9df6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reduce\n# Reduce is a commonly used action. It takes an RDD as input and returns a single reduced value. As in Python, to obtain the result, it repeatedly applies a reduce operator to the elements of an RDD. The reduce operator is a function that takes two elements as input and returns one as output.\n\n# Workers execute reduce in parallel: Each executor reduces the data local to it, and the results from all executors are combined. It is therefore extermely important that the reduce operator is both commutative and associative as the execution order cannot be known in advance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccab8886-8902-4b1f-9509-afa2cedd3aa3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reduce rdd to the sum of its elements\nrdd.reduce(lambda x,y:x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0cae1ed-ab3a-43c5-b73e-fdb5eb9c3c37"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[183]: 435</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[183]: 435</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Here is a slightly more elaborate example. Try to figure out what this code is doing, and the result is going to be. Then run the code to check yourself."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1904f8c-6ac1-4b52-b645-1fb3797d3190"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["words=['this','is','the','best','linux','ever','Saikiran']\nwordRDD=sc.parallelize(words)\nwordRDD.reduce(lambda w,v: w if len(w)>len(v) else v)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b8f3b5a-dbe9-4aff-b1f6-7f5cff4c3756"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[186]: &#39;Saikiran&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[186]: &#39;Saikiran&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Simple Map and Reduce pipelines\n# Compute the sum of squares. Sequential syntax:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c4d5dc0-48f2-47b1-88ca-79be4a8f7e74"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["Squares=rdd.map(lambda x:x*x)\nSquares.reduce(lambda x,y:x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"259257d5-197c-4be6-aecf-54070814575f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[210]: 7500000</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[210]: 7500000</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#cascading is more compact and \"functional\": \nrdd.map(lambda x:x*x).reduce(lambda x,y:x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27a7358d-1c3b-4f19-8113-be97b17ffe3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[212]: 7500000</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[212]: 7500000</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Extracting basic information about an RDD\n# RDD's typically have hundreds of thousands of elements. It usually makes no sense to print out the content of a whole RDD. Here are some ways to get manageable amounts of information about an RDD.\n\n# Create an RDD of length n which is a repetition of the pattern 1,2,3,4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70f94284-29ce-497e-9fcb-fe8bb38f3c47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["n = 1000000\nrdd = sc.parallelize([1,2,3,4]*int(n/4))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42393ab7-d564-47ee-8478-6d12b33797a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use the count() action to find the number of elements in the RDD\nrdd.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7218809-4e5e-4a30-81e0-2e5d1abfb16f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[215]: 1000000</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[215]: 1000000</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Get the first few elements of an RDD\n# Both first() and take() are actions\nprint('first element=', rdd.first())\nprint('first 5 elements = ', rdd.take(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"169fc337-9548-40f7-93f5-2bc6ee77d2d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">first element= 1\nfirst 5 elements =  [1, 2, 3, 4, 1, 2, 3, 4, 1, 2]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">first element= 1\nfirst 5 elements =  [1, 2, 3, 4, 1, 2, 3, 4, 1, 2]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Sampling an RDD\n# RDDs are often very large.\n# Aggregates, such as averages, can be approximated efficiently by using a sample.\n# Sampling is done in parallel and requires limited computation.\n\n\n# The transformation sample(withReplacement, p) generates a sample of the elements of the RDD. where\n\n# withReplacement is a boolean flag indicating whether or not a an element in the RDD can be sampled more than once.\n# p is the probability of accepting each element into the sample. Note that as the sampling is performed independently in each partition, the number of elements in the sample changes from sample to sample."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c85af7f-f9eb-4d68-a64c-5c31523659d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# get a sample whose expected size is m\n# Note that the size of the sample is different in different runs\nm=5.\nprint('sample1=', rdd.sample(False, m/n).collect()) \nprint('sample2=', rdd.sample(False, m/n).collect())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"372960b4-f292-4d36-95fe-b9f5272fc619"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">sample1= [4, 1, 2, 1, 4, 4]\nsample2= [1, 4, 4, 3]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sample1= [4, 1, 2, 1, 4, 4]\nsample2= [1, 4, 4, 3]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Filtering an RDD\n# The transformation filter(func) returns a new RDD formed by selecting those elements of the source RDD on which func returns true"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9d30baf-d597-40ad-9622-734898021598"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print('the number of elements in rdd that are > 3 =', rdd.filter(lambda n: n > 3).count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a90707b5-321a-46b9-b383-3d8e41206fea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">the number of elements in rdd that are &gt; 3 = 250000\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">the number of elements in rdd that are &gt; 3 = 250000\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Removing duplicate elements from an RDD\n# The transformation distinct() returns a new RDD that contains the distinct elements of the source RDD.\n\n# This operation requires a shuffle in order to detect duplication across partitions (we'll discuss the implications of this later in the class)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ff4535c-c146-4729-89d4-53fddf5c1894"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Remove duplicate element in DuplicateRDD, we get distinct RDD\nDuplicateRDD = sc.parallelize([1, 1, 2, 2, 3, 3])\nprint('DuplicateRDD=', DuplicateRDD.collect())\nprint('DistinctRDD = ', DuplicateRDD.distinct().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3785db26-0fea-430d-8358-b5dd8cd60d01"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">DuplicateRDD= [1, 1, 2, 2, 3, 3]\nDistinctRDD =  [1, 2, 3]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DuplicateRDD= [1, 1, 2, 2, 3, 3]\nDistinctRDD =  [1, 2, 3]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Flat Map\n# The transformation flatMap(func) is similar to map, but the result is \"flattened\" before being returned."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e66dbd3-f564-4feb-b619-430771ca8877"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["text=[\"you are my sunshine\",\"my only sunshine\"]\ntext_file = sc.parallelize(text)\n\n# map each line in text to a list of words\nprint('map:',text_file.map(lambda line: line.split(\" \")).collect())\n\n# create a single list of words by combining the words from all of the lines\nprint('flatmap:',text_file.flatMap(lambda line: line.split(\" \")).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c761ae8b-57d9-4e16-a2c5-a6847b171e68"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">map: [[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\nflatmap: [&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;, &#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">map: [[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\nflatmap: [&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;, &#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Exercise 1\n# Write a Spark program that uses the map() transformation followed by the collect() action to compute and output the list of cubes of integer numbers in a given range."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ba1c1ab-a6a3-44b9-9294-0da239e872f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# A sample range of numbers. Experiment with different ones.\nr = range(5)\n\n# Create an RDD from r\nnumsRDD = sc.parallelize(r)\n\n# Use map to conver it to an RDD consisting of a sequence of cubes followed by collect() to materialize the result\n# as a list\nnumsRDD.map(lambda x: x*x*x).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca5c05e8-c4d5-487a-ba99-729f655729a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[36]: [0, 1, 8, 27, 64]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: [0, 1, 8, 27, 64]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 2\n# Write a Spark program that uses filter() followed by collect() to output the list of all objects which are positive integers in the given list of objects."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b79979b5-222e-4338-9430-3fdf514b78ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Sample input list of objects\nlst = [1234, '666', 'hi, there!', -23, 0, 'bye', 1]\n# Your code goes here\nlstRDD = sc.parallelize(lst)\nlstRDD.filter(lambda x: type(x) is int and x > 0).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bcd9836-5149-4a81-a9bd-1691449f583e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[38]: [1234, 1]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: [1234, 1]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 3\n# Combine the filter() transformation from the previous exercise with the map() transformation to obtain the list of cubes of all positive numbers found in an input list of objects."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ee569bd-5449-451c-aee8-95000b124e72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Sample input list of objects\nlst = [1234, '666', 'hi, there!', -23, 0, 'bye', 1]\n# Your code goes here\nlstRDD.filter(lambda x: type(x) is int and x > 0).map(lambda x : x*x*x).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39d36d05-5b83-4d31-aa34-aa205c559074"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[40]: [1879080904, 1]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: [1879080904, 1]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 4\n# Given an in-memory collection of words, write a Spark program that computes and outputs the longest one.\n# A sample collection. Feel free to modify as you wish.\nwords=['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine','Saikiranlearnspython']\n# Your code goes here\nsc.parallelize(words).reduce(lambda p, w: p if len(p) > len(w) else w)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3628f0c-4891-49fb-a1e8-5d5c4b76c389"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[49]: &#39;Saikiranlearnspython&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[49]: &#39;Saikiranlearnspython&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 5\n# Given an in-memory collection of strings, write a Spark program that computes the average of their length. Hint: One possible implementation will first map every word to its length, reduce it to the sum, and then divide it by the length of the list. Note that for larger datasets, it is better to use the count() action to count the number of elements."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5fca340-94df-498c-af78-220334472821"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# A sample collection. Feel free to modify as you wish.\nwords=['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine']\n# Your code goes here\nwordsRDD = sc.parallelize(words)\nsum = wordsRDD.map(lambda x: len(x)).reduce(lambda acc, y: acc+y)\nprint(sum)\ncnt = wordsRDD.count()\navg = sum / cnt\nprint('Avg = {0:.1f}'.format(avg))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47262381-1e7c-4af3-be9c-25658ae30344"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">30\nAvg = 4.3\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">30\nAvg = 4.3\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 6\n# Modify the above program to only include the words longer than 2 characters. Hint: Use filter transformation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe93f420-cfec-48fd-9ce0-9c82e749b9f2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# A sample collection. Feel free to modify as you wish.\nwords=['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine']\n# Your code goes here\nlensRDD = sc.parallelize(words).map(lambda x: len(x)).filter(lambda x: x > 2)\nsum = lensRDD.reduce(lambda acc, y: acc+y)\ncnt = lensRDD.count()\navg = sum / cnt\nprint('Avg = {0:.1f}'.format(avg))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c672b33-c14d-429d-be67-efbc8d12a042"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Avg = 5.2\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Avg = 5.2\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 7\n# Repeat Exercise 4 but assume the input is a list of sentences, and not individual words. Hint: Use the flatmap() transformation discussed above to flatten the list of words returned by split(), and then apply reduce() as in Exercise 4. Do the same for Exercises 5 and 6."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f529276-72ca-45ef-8a38-01fd185b80b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\n# A sample collection. Feel free to modify as you wish.\ntext=[\"you are my sunshine\",\"my only sunshine\"]\n# Your code goes here\n\nwordsRDD = sc.parallelize(text).map(lambda s: s.split())\nwordsRDD.flatMap(lambda s: s).reduce(lambda p, w: p if len(p) > len(w) else w)\nprint(wordsRDD.collect())\n\n# toDebugString is useful to print out the lineage graph for debugging.\nprint(wordsRDD.toDebugString().decode('utf-8'))\n\n# sometimes useful to print out help info\nhelp(pyspark.RDD.flatMap)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3968e91-842b-4b27-a52e-113a7e7196ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\n(8) PythonRDD[100] at collect at &lt;command-1710337403652345&gt;:8 []\n |  ParallelCollectionRDD[98] at readRDDFromInputStream at PythonRDD.scala:413 []\nHelp on function flatMap in module pyspark.rdd:\n\nflatMap(self, f, preservesPartitioning=False)\n    Return a new RDD by first applying a function to all elements of this\n    RDD, and then flattening the results.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])\n    &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n    [1, 1, 1, 2, 2, 3]\n    &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n    [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\n(8) PythonRDD[100] at collect at &lt;command-1710337403652345&gt;:8 []\n  ParallelCollectionRDD[98] at readRDDFromInputStream at PythonRDD.scala:413 []\nHelp on function flatMap in module pyspark.rdd:\n\nflatMap(self, f, preservesPartitioning=False)\n    Return a new RDD by first applying a function to all elements of this\n    RDD, and then flattening the results.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])\n    &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n    [1, 1, 1, 2, 2, 3]\n    &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n    [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["input_data = [\"Python Pool\",\n        \"Latracal Solutions\",\n        \"Python pool is best\",\n        \"Basic command in python\"]\nrdd=spark.sparkContext.parallelize(input_data)\nrdd2=rdd.flatMap(lambda x: x.split(\" \"))\nlist(rdd2.collect())\n#    print(ele)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f314087d-2046-456b-8da9-679ba10ca43f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[88]: [&#39;Python&#39;,\n &#39;Pool&#39;,\n &#39;Latracal&#39;,\n &#39;Solutions&#39;,\n &#39;Python&#39;,\n &#39;pool&#39;,\n &#39;is&#39;,\n &#39;best&#39;,\n &#39;Basic&#39;,\n &#39;command&#39;,\n &#39;in&#39;,\n &#39;python&#39;,\n &#39;sa&#39;,\n &#39;i&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[88]: [&#39;Python&#39;,\n &#39;Pool&#39;,\n &#39;Latracal&#39;,\n &#39;Solutions&#39;,\n &#39;Python&#39;,\n &#39;pool&#39;,\n &#39;is&#39;,\n &#39;best&#39;,\n &#39;Basic&#39;,\n &#39;command&#39;,\n &#39;in&#39;,\n &#39;python&#39;,\n &#39;sa&#39;,\n &#39;i&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["text=[\"you are my sunshine\",\"my only sunshine\"]\nrdd = sc.parallelize(text)\nrdd.flatMap(lambda x: x.split())\\\n     .map(lambda x: len(x))\\\n     .map(lambda x: (x, 1))\\\n     .reduceByKey(lambda x, y: x + y)\\\n     .collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"783c241a-d085-4a8d-bbf7-94654dbae1d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[93]: [(8, 2), (2, 2), (3, 2), (4, 1)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[93]: [(8, 2), (2, 2), (3, 2), (4, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["text=[\"you are my sunshine\",\"my only sunshine\"]\nrdd = sc.parallelize(text)\nrdd.flatMap(lambda x: x.split()).map(lambda x: len(x)).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d79207b-bf49-4075-bf4f-e8f9cd6bd955"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[97]: [(8, 2), (2, 2), (3, 2), (4, 1)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[97]: [(8, 2), (2, 2), (3, 2), (4, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21f36cdc-ff13-4df9-b77e-6b2c44bf2124"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#flat file storage \nrdd = sc.textFile(\"dbfs:/FileStore/shared_uploads/csaikiran482@gmail.com/blogtexts\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f16cc9b-14dd-449b-9761-74637bc4b3dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b2c6e22-3c8b-44b5-b148-aed482553486"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[78]: [&#39;Think of it for a moment – 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.&#39;,\n &#39;&#39;,\n &#39;Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.&#39;,\n &#39;&#39;,\n &#39;On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[78]: [&#39;Think of it for a moment – 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.&#39;,\n &#39;&#39;,\n &#39;Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.&#39;,\n &#39;&#39;,\n &#39;On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def Func(lines):\n      lines = lines.lower()\n      lines = lines.split()\n      return lines\nrdd1 = rdd.map(Func)\nrdd1.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80ed2c04-cef9-4789-91ef-f3771f838295"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[79]: [[&#39;think&#39;,\n  &#39;of&#39;,\n  &#39;it&#39;,\n  &#39;for&#39;,\n  &#39;a&#39;,\n  &#39;moment&#39;,\n  &#39;–&#39;,\n  &#39;1&#39;,\n  &#39;qunitillion&#39;,\n  &#39;=&#39;,\n  &#39;1&#39;,\n  &#39;million&#39;,\n  &#39;billion!&#39;,\n  &#39;can&#39;,\n  &#39;you&#39;,\n  &#39;imagine&#39;,\n  &#39;how&#39;,\n  &#39;many&#39;,\n  &#39;drives&#39;,\n  &#39;/&#39;,\n  &#39;cds&#39;,\n  &#39;/&#39;,\n  &#39;blue-ray&#39;,\n  &#39;dvds&#39;,\n  &#39;would&#39;,\n  &#39;be&#39;,\n  &#39;required&#39;,\n  &#39;to&#39;,\n  &#39;store&#39;,\n  &#39;them?&#39;,\n  &#39;it&#39;,\n  &#39;is&#39;,\n  &#39;difficult&#39;,\n  &#39;to&#39;,\n  &#39;imagine&#39;,\n  &#39;this&#39;,\n  &#39;scale&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation&#39;,\n  &#39;even&#39;,\n  &#39;as&#39;,\n  &#39;a&#39;,\n  &#39;data&#39;,\n  &#39;science&#39;,\n  &#39;professional.&#39;,\n  &#39;while&#39;,\n  &#39;this&#39;,\n  &#39;pace&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation&#39;,\n  &#39;is&#39;,\n  &#39;very&#39;,\n  &#39;exciting,&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;created&#39;,\n  &#39;entirely&#39;,\n  &#39;new&#39;,\n  &#39;set&#39;,\n  &#39;of&#39;,\n  &#39;challenges&#39;,\n  &#39;and&#39;,\n  &#39;has&#39;,\n  &#39;forced&#39;,\n  &#39;us&#39;,\n  &#39;to&#39;,\n  &#39;find&#39;,\n  &#39;new&#39;,\n  &#39;ways&#39;,\n  &#39;to&#39;,\n  &#39;handle&#39;,\n  &#39;big&#39;,\n  &#39;huge&#39;,\n  &#39;data&#39;,\n  &#39;effectively.&#39;],\n [],\n [&#39;big&#39;,\n  &#39;data&#39;,\n  &#39;is&#39;,\n  &#39;not&#39;,\n  &#39;a&#39;,\n  &#39;new&#39;,\n  &#39;phenomena.&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;been&#39;,\n  &#39;around&#39;,\n  &#39;for&#39;,\n  &#39;a&#39;,\n  &#39;while&#39;,\n  &#39;now.&#39;,\n  &#39;however,&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;become&#39;,\n  &#39;really&#39;,\n  &#39;important&#39;,\n  &#39;with&#39;,\n  &#39;this&#39;,\n  &#39;pace&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation.&#39;,\n  &#39;in&#39;,\n  &#39;past,&#39;,\n  &#39;several&#39;,\n  &#39;systems&#39;,\n  &#39;were&#39;,\n  &#39;developed&#39;,\n  &#39;for&#39;,\n  &#39;processing&#39;,\n  &#39;big&#39;,\n  &#39;data.&#39;,\n  &#39;most&#39;,\n  &#39;of&#39;,\n  &#39;them&#39;,\n  &#39;were&#39;,\n  &#39;based&#39;,\n  &#39;on&#39;,\n  &#39;mapreduce&#39;,\n  &#39;framework.&#39;,\n  &#39;these&#39;,\n  &#39;frameworks&#39;,\n  &#39;typically&#39;,\n  &#39;rely&#39;,\n  &#39;on&#39;,\n  &#39;use&#39;,\n  &#39;of&#39;,\n  &#39;hard&#39;,\n  &#39;disk&#39;,\n  &#39;for&#39;,\n  &#39;saving&#39;,\n  &#39;and&#39;,\n  &#39;retrieving&#39;,\n  &#39;the&#39;,\n  &#39;results.&#39;,\n  &#39;however,&#39;,\n  &#39;this&#39;,\n  &#39;turns&#39;,\n  &#39;out&#39;,\n  &#39;to&#39;,\n  &#39;be&#39;,\n  &#39;very&#39;,\n  &#39;costly&#39;,\n  &#39;in&#39;,\n  &#39;terms&#39;,\n  &#39;of&#39;,\n  &#39;time&#39;,\n  &#39;and&#39;,\n  &#39;speed.&#39;],\n [],\n [&#39;on&#39;,\n  &#39;the&#39;,\n  &#39;other&#39;,\n  &#39;hand,&#39;,\n  &#39;organizations&#39;,\n  &#39;have&#39;,\n  &#39;never&#39;,\n  &#39;been&#39;,\n  &#39;more&#39;,\n  &#39;hungrier&#39;,\n  &#39;to&#39;,\n  &#39;add&#39;,\n  &#39;a&#39;,\n  &#39;competitive&#39;,\n  &#39;differentiation&#39;,\n  &#39;through&#39;,\n  &#39;understanding&#39;,\n  &#39;this&#39;,\n  &#39;data&#39;,\n  &#39;and&#39;,\n  &#39;offering&#39;,\n  &#39;its&#39;,\n  &#39;customer&#39;,\n  &#39;a&#39;,\n  &#39;much&#39;,\n  &#39;better&#39;,\n  &#39;experience.&#39;,\n  &#39;imagine&#39;,\n  &#39;how&#39;,\n  &#39;valuable&#39;,\n  &#39;would&#39;,\n  &#39;be&#39;,\n  &#39;facebook,&#39;,\n  &#39;if&#39;,\n  &#39;it&#39;,\n  &#39;did&#39;,\n  &#39;not&#39;,\n  &#39;understand&#39;,\n  &#39;your&#39;,\n  &#39;interests&#39;,\n  &#39;well?&#39;,\n  &#39;the&#39;,\n  &#39;traditional&#39;,\n  &#39;hard&#39;,\n  &#39;disk&#39;,\n  &#39;based&#39;,\n  &#39;mapreduce&#39;,\n  &#39;kind&#39;,\n  &#39;of&#39;,\n  &#39;frameworks&#39;,\n  &#39;do&#39;,\n  &#39;not&#39;,\n  &#39;help&#39;,\n  &#39;much&#39;,\n  &#39;to&#39;,\n  &#39;address&#39;,\n  &#39;this&#39;,\n  &#39;challenge.&#39;]]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[79]: [[&#39;think&#39;,\n  &#39;of&#39;,\n  &#39;it&#39;,\n  &#39;for&#39;,\n  &#39;a&#39;,\n  &#39;moment&#39;,\n  &#39;–&#39;,\n  &#39;1&#39;,\n  &#39;qunitillion&#39;,\n  &#39;=&#39;,\n  &#39;1&#39;,\n  &#39;million&#39;,\n  &#39;billion!&#39;,\n  &#39;can&#39;,\n  &#39;you&#39;,\n  &#39;imagine&#39;,\n  &#39;how&#39;,\n  &#39;many&#39;,\n  &#39;drives&#39;,\n  &#39;/&#39;,\n  &#39;cds&#39;,\n  &#39;/&#39;,\n  &#39;blue-ray&#39;,\n  &#39;dvds&#39;,\n  &#39;would&#39;,\n  &#39;be&#39;,\n  &#39;required&#39;,\n  &#39;to&#39;,\n  &#39;store&#39;,\n  &#39;them?&#39;,\n  &#39;it&#39;,\n  &#39;is&#39;,\n  &#39;difficult&#39;,\n  &#39;to&#39;,\n  &#39;imagine&#39;,\n  &#39;this&#39;,\n  &#39;scale&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation&#39;,\n  &#39;even&#39;,\n  &#39;as&#39;,\n  &#39;a&#39;,\n  &#39;data&#39;,\n  &#39;science&#39;,\n  &#39;professional.&#39;,\n  &#39;while&#39;,\n  &#39;this&#39;,\n  &#39;pace&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation&#39;,\n  &#39;is&#39;,\n  &#39;very&#39;,\n  &#39;exciting,&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;created&#39;,\n  &#39;entirely&#39;,\n  &#39;new&#39;,\n  &#39;set&#39;,\n  &#39;of&#39;,\n  &#39;challenges&#39;,\n  &#39;and&#39;,\n  &#39;has&#39;,\n  &#39;forced&#39;,\n  &#39;us&#39;,\n  &#39;to&#39;,\n  &#39;find&#39;,\n  &#39;new&#39;,\n  &#39;ways&#39;,\n  &#39;to&#39;,\n  &#39;handle&#39;,\n  &#39;big&#39;,\n  &#39;huge&#39;,\n  &#39;data&#39;,\n  &#39;effectively.&#39;],\n [],\n [&#39;big&#39;,\n  &#39;data&#39;,\n  &#39;is&#39;,\n  &#39;not&#39;,\n  &#39;a&#39;,\n  &#39;new&#39;,\n  &#39;phenomena.&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;been&#39;,\n  &#39;around&#39;,\n  &#39;for&#39;,\n  &#39;a&#39;,\n  &#39;while&#39;,\n  &#39;now.&#39;,\n  &#39;however,&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;become&#39;,\n  &#39;really&#39;,\n  &#39;important&#39;,\n  &#39;with&#39;,\n  &#39;this&#39;,\n  &#39;pace&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation.&#39;,\n  &#39;in&#39;,\n  &#39;past,&#39;,\n  &#39;several&#39;,\n  &#39;systems&#39;,\n  &#39;were&#39;,\n  &#39;developed&#39;,\n  &#39;for&#39;,\n  &#39;processing&#39;,\n  &#39;big&#39;,\n  &#39;data.&#39;,\n  &#39;most&#39;,\n  &#39;of&#39;,\n  &#39;them&#39;,\n  &#39;were&#39;,\n  &#39;based&#39;,\n  &#39;on&#39;,\n  &#39;mapreduce&#39;,\n  &#39;framework.&#39;,\n  &#39;these&#39;,\n  &#39;frameworks&#39;,\n  &#39;typically&#39;,\n  &#39;rely&#39;,\n  &#39;on&#39;,\n  &#39;use&#39;,\n  &#39;of&#39;,\n  &#39;hard&#39;,\n  &#39;disk&#39;,\n  &#39;for&#39;,\n  &#39;saving&#39;,\n  &#39;and&#39;,\n  &#39;retrieving&#39;,\n  &#39;the&#39;,\n  &#39;results.&#39;,\n  &#39;however,&#39;,\n  &#39;this&#39;,\n  &#39;turns&#39;,\n  &#39;out&#39;,\n  &#39;to&#39;,\n  &#39;be&#39;,\n  &#39;very&#39;,\n  &#39;costly&#39;,\n  &#39;in&#39;,\n  &#39;terms&#39;,\n  &#39;of&#39;,\n  &#39;time&#39;,\n  &#39;and&#39;,\n  &#39;speed.&#39;],\n [],\n [&#39;on&#39;,\n  &#39;the&#39;,\n  &#39;other&#39;,\n  &#39;hand,&#39;,\n  &#39;organizations&#39;,\n  &#39;have&#39;,\n  &#39;never&#39;,\n  &#39;been&#39;,\n  &#39;more&#39;,\n  &#39;hungrier&#39;,\n  &#39;to&#39;,\n  &#39;add&#39;,\n  &#39;a&#39;,\n  &#39;competitive&#39;,\n  &#39;differentiation&#39;,\n  &#39;through&#39;,\n  &#39;understanding&#39;,\n  &#39;this&#39;,\n  &#39;data&#39;,\n  &#39;and&#39;,\n  &#39;offering&#39;,\n  &#39;its&#39;,\n  &#39;customer&#39;,\n  &#39;a&#39;,\n  &#39;much&#39;,\n  &#39;better&#39;,\n  &#39;experience.&#39;,\n  &#39;imagine&#39;,\n  &#39;how&#39;,\n  &#39;valuable&#39;,\n  &#39;would&#39;,\n  &#39;be&#39;,\n  &#39;facebook,&#39;,\n  &#39;if&#39;,\n  &#39;it&#39;,\n  &#39;did&#39;,\n  &#39;not&#39;,\n  &#39;understand&#39;,\n  &#39;your&#39;,\n  &#39;interests&#39;,\n  &#39;well?&#39;,\n  &#39;the&#39;,\n  &#39;traditional&#39;,\n  &#39;hard&#39;,\n  &#39;disk&#39;,\n  &#39;based&#39;,\n  &#39;mapreduce&#39;,\n  &#39;kind&#39;,\n  &#39;of&#39;,\n  &#39;frameworks&#39;,\n  &#39;do&#39;,\n  &#39;not&#39;,\n  &#39;help&#39;,\n  &#39;much&#39;,\n  &#39;to&#39;,\n  &#39;address&#39;,\n  &#39;this&#39;,\n  &#39;challenge.&#39;]]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Output is too long so, I have just attached a snippet of it. We can also see that our output is not flat (it’s a nested list). So for getting the flat output, we need to apply a transformation which will flatten the output, The transformation “flatMap” will help here:\n\n# The “flatMap” transformation will return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. This is the main difference between the “flatMap” and map transformations. Let’s apply a “flatMap” transformation on “rdd” , then take the result of this transformation in “rdd2” and print the result after applying this transformation.\n\nrdd2 = rdd.flatMap(Func)\nrdd2.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55ad8b16-f585-4cdb-a5b8-b4aa36231fa8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[80]: [&#39;think&#39;, &#39;of&#39;, &#39;it&#39;, &#39;for&#39;, &#39;a&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[80]: [&#39;think&#39;, &#39;of&#39;, &#39;it&#39;, &#39;for&#39;, &#39;a&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: filter\n# Q2: Next, I want to remove the words, which are not necessary to analyze this text. We call these words as “stop words”; Stop words do not add much value in a text. For example, “is”, “am”, “are” and “the” are few examples of stop words.\n\n# Solution: To remove the stop words, we can use a “filter” transformation which will return a new RDD containing only the elements that satisfy given condition(s). Lets apply “filter” transformation on “rdd2” and get words which are not stop words and get the result in “rdd3”. To do that:\n\n#  We need to define the list of stop words in a variable called “stopwords” ( Here, I am selecting only a few words in stop words list instead of all the words).\n#  Apply “filter” on “rdd2” (Check if individual words of “rdd2” are in the “stopwords” list or not ).\n# We can check first 10 elements of “rdd3” by applying take action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86acb2f3-bd29-4855-a9d9-f7fb69d582be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["stopwords = ['is','am','are','the','for','a']\nrdd3= rdd2.filter(lambda x: x not in stopwords)\nrdd3.take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6afe9a11-0f38-49a8-b511-7d6a2f27d191"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[82]: [&#39;think&#39;, &#39;of&#39;, &#39;it&#39;, &#39;moment&#39;, &#39;–&#39;, &#39;1&#39;, &#39;qunitillion&#39;, &#39;=&#39;, &#39;1&#39;, &#39;million&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[82]: [&#39;think&#39;, &#39;of&#39;, &#39;it&#39;, &#39;moment&#39;, &#39;–&#39;, &#39;1&#39;, &#39;qunitillion&#39;, &#39;=&#39;, &#39;1&#39;, &#39;million&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: groupBy\n# Q3: After getting the results into rdd3, we want to group the words in rdd3 based on which letters they start with. For example, suppose I want to group each word of rdd3 based on first 3 characters.\n\n# Solution: The “groupBy”  transformation will group the data in the original RDD. It creates a set of key value pairs, where the key is output of a user function, and the value is all items for which the function yields this key.\n\n# We have to pass a function (in this case, I am using a lambda function) inside the “groupBy” which will take the first 3 characters of each word in “rdd3”.\n# The key is the first 3 characters and value is all the words which start with these 3 characters.\n# After applying “groupBy” function, we store the transformed result in “rdd4” (RDDs are immutable – remember!). To view “rdd4”, we can print first (key, value) elements in “rdd4”."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6057fc2f-0fdc-4430-ab6c-15459c2b1619"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd4 = rdd3.groupBy(lambda w: w[0:3])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a17e8a4-ef99-4984-adb6-ba241d06b941"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print([(k, list(k)) for (k,v) in rdd4.take(10)])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7edf6973-a6f7-4b6c-b84f-ea6464d5c9c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[(&#39;thi&#39;, [&#39;t&#39;, &#39;h&#39;, &#39;i&#39;]), (&#39;of&#39;, [&#39;o&#39;, &#39;f&#39;]), (&#39;1&#39;, [&#39;1&#39;]), (&#39;qun&#39;, [&#39;q&#39;, &#39;u&#39;, &#39;n&#39;]), (&#39;=&#39;, [&#39;=&#39;]), (&#39;mil&#39;, [&#39;m&#39;, &#39;i&#39;, &#39;l&#39;]), (&#39;bil&#39;, [&#39;b&#39;, &#39;i&#39;, &#39;l&#39;]), (&#39;ima&#39;, [&#39;i&#39;, &#39;m&#39;, &#39;a&#39;]), (&#39;cds&#39;, [&#39;c&#39;, &#39;d&#39;, &#39;s&#39;]), (&#39;dvd&#39;, [&#39;d&#39;, &#39;v&#39;, &#39;d&#39;])]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;thi&#39;, [&#39;t&#39;, &#39;h&#39;, &#39;i&#39;]), (&#39;of&#39;, [&#39;o&#39;, &#39;f&#39;]), (&#39;1&#39;, [&#39;1&#39;]), (&#39;qun&#39;, [&#39;q&#39;, &#39;u&#39;, &#39;n&#39;]), (&#39;=&#39;, [&#39;=&#39;]), (&#39;mil&#39;, [&#39;m&#39;, &#39;i&#39;, &#39;l&#39;]), (&#39;bil&#39;, [&#39;b&#39;, &#39;i&#39;, &#39;l&#39;]), (&#39;ima&#39;, [&#39;i&#39;, &#39;m&#39;, &#39;a&#39;]), (&#39;cds&#39;, [&#39;c&#39;, &#39;d&#39;, &#39;s&#39;]), (&#39;dvd&#39;, [&#39;d&#39;, &#39;v&#39;, &#39;d&#39;])]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: groupBy\n# Q3: After getting the results into rdd3, we want to group the words in rdd3 based on which letters they start with. For example, suppose I want to group each word of rdd3 based on first 3 characters.\n\n# Solution: The “groupBy”  transformation will group the data in the original RDD. It creates a set of key value pairs, where the key is output of a user function, and the value is all items for which the function yields this key.\n\n# We have to pass a function (in this case, I am using a lambda function) inside the “groupBy” which will take the first 3 characters of each word in “rdd3”.\n# The key is the first 3 characters and value is all the words which start with these 3 characters.\n# After applying “groupBy” function, we store the transformed result in “rdd4” (RDDs are immutable – remember!). To view “rdd4”, we can print first (key, value) elements in “rdd4”."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ff3c344-b316-4a59-87a7-a2fca9041c04"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_mapped = rdd3.map(lambda x: (x,1))\nrdd3_grouped = rdd3_mapped.groupByKey()\nprint(list((j[0], list(j[1])) for j in rdd3_grouped.take(5)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e46c6bd9-caef-4f06-8754-afbb429237e3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[(&#39;think&#39;, [1, 1]), (&#39;of&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (&#39;1&#39;, [1, 1]), (&#39;qunitillion&#39;, [1]), (&#39;=&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;think&#39;, [1, 1]), (&#39;of&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (&#39;1&#39;, [1, 1]), (&#39;qunitillion&#39;, [1]), (&#39;=&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: groupByKey / reduceByKey \n# Q4: What if we want to calculate how many times each word is coming in corpus ?\n\n# Solution: We can apply the “groupByKey” / “reduceByKey” transformations on (key,val) pair RDD. The “groupByKey” will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values.\n\n# To use “groupbyKey” / “reduceByKey” transformation to find the frequencies of each words, you can follow the steps below:\n\n# A (key,val) pair RDD is required; In this (key,val) pair RDD, key is the word and val is 1 for each word in RDD (1 represents the number for the each word in “rdd3”).\n# To apply “groupbyKey” / “reduceByKey” on “rdd3”, we need to first convert “rdd3” to (key,val) pair RDD.\n \n\n# Let’s see, how to convert “rdd3” to new mapped (key,val) RDD. And then we can apply “groupbyKey” / “reduceByKey” transformation on this RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86ff0113-786f-478a-832a-7b101b81679a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_mapped = rdd3.map(lambda x: (x,1))\nrdd3_grouped = rdd3_mapped.groupByKey()\nprint(list((j[0], list(j[1])) for j in rdd3_grouped.take(5)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d5cd53a-1b1d-4eac-a83c-b8d3aa498502"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[(&#39;think&#39;, [1, 1]), (&#39;of&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (&#39;1&#39;, [1, 1]), (&#39;qunitillion&#39;, [1]), (&#39;=&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;think&#39;, [1, 1]), (&#39;of&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (&#39;1&#39;, [1, 1]), (&#39;qunitillion&#39;, [1]), (&#39;=&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# After seeing the result of the above code, I rechecked the corpus to know, how many times the word ‘manager’ is there, so I found that ‘manager’ is written more then once. I figure out that there are more words like ‘manager.’ , ‘manager,’ and ”manager:’. Let’s filter ‘manager,’ in “rdd3”."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c169578-a79c-466d-a0fe-42078c8d20c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.filter(lambda x: x == 'manager,').collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a2aad74-b3f2-4ff8-9d74-082fef46c1fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[134]: [&#39;manager,&#39;, &#39;manager,&#39;, &#39;manager,&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[134]: [&#39;manager,&#39;, &#39;manager,&#39;, &#39;manager,&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# We can see that in above output, we have multiple words with ‘manager’ in our corpus. To overcome this situation we can do several things. We could apply a regular expression to remove unnecessary punctuation from the words. For the purpose of this article, I am skipping that part.\n\n# Until now we have not calculated the frequencies / counts of each words. Let’s proceed further :"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49953805-a7dd-48bd-aa07-43ba9cf03c1b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_freq_of_words = rdd3_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91c5147d-6e31-40ab-a7e0-ee42b29abde8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# In the above code, I first applied “mapValues” transformation on “rdd3_grouped”. The “mapValues” (only applicable on pair RDD) transformation is like a map (can be applied on any RDD) transform but it has one difference that when we apply map transform on pair RDD we can access the key and value both of this RDD but in case of “mapValues” transformation, it will transform the values by applying some function and key will not be affected. So for example, in above code I applied sum, which will calculate the sum (counts) for the each word.\n\n# After applying “mapValues”  transformation I want to sort the words based on their frequencies so for doing that I am first converting a ( word, frequency ) pair to ( frequency,word ) so that our key and values will be interchanged then, I will apply a sorting based on key and then get a result in “rdd3_freq_of_words”. We can see that 10 most frequent words I used in my previous blog by applying “take” action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5c8c2e7-79b5-49a5-a4f9-2f6886f6d76c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_freq_of_words.take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e6d25c7-cd92-4d2b-8940-598cf8a47587"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[138]: [(164, &#39;to&#39;),\n (143, &#39;in&#39;),\n (122, &#39;of&#39;),\n (106, &#39;and&#39;),\n (103, &#39;we&#39;),\n (69, &#39;spark&#39;),\n (64, &#39;this&#39;),\n (63, &#39;data&#39;),\n (55, &#39;can&#39;),\n (52, &#39;apache&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[138]: [(164, &#39;to&#39;),\n (143, &#39;in&#39;),\n (122, &#39;of&#39;),\n (106, &#39;and&#39;),\n (103, &#39;we&#39;),\n (69, &#39;spark&#39;),\n (64, &#39;this&#39;),\n (63, &#39;data&#39;),\n (55, &#39;can&#39;),\n (52, &#39;apache&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Above output shows that I used words spark 69 times and Apache 52 times in my previous blog.\n\n \n\n# We can also use “reduceByKey” transformation for counting the frequencies of each word in (key,value) pair RDD. Lets see how will we do this."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f446d97-952e-435b-b21e-fa75fec7ba2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9476c67c-8657-41b1-a4f8-af6d09f9252d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[140]: [(164, &#39;to&#39;),\n (143, &#39;in&#39;),\n (122, &#39;of&#39;),\n (106, &#39;and&#39;),\n (103, &#39;we&#39;),\n (69, &#39;spark&#39;),\n (64, &#39;this&#39;),\n (63, &#39;data&#39;),\n (55, &#39;can&#39;),\n (52, &#39;apache&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[140]: [(164, &#39;to&#39;),\n (143, &#39;in&#39;),\n (122, &#39;of&#39;),\n (106, &#39;and&#39;),\n (103, &#39;we&#39;),\n (69, &#39;spark&#39;),\n (64, &#39;this&#39;),\n (63, &#39;data&#39;),\n (55, &#39;can&#39;),\n (52, &#39;apache&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: mapPartitions\n# Q5: How do I perform a task (say count the words ‘spark’ and ‘apache’ in rdd3) separatly on each partition and get the output of the task performed in these partition ?\n# Soltion: We can do this by applying “mapPartitions” transformation. The “mapPartitions” is like a map transformation but runs separately on different partitions of a RDD. So, for counting the frequencies of words ‘spark’ and ‘apache’ in each partition of RDD, you can follow the steps:\n\n# Create a function called “func” which will count the frequencies for these words\n#  Then, pass the function defined in step1 to the “mapPartitions” transformation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18784636-3f41-45c8-ae55-b85125a0988e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def func(iterator):\n  count_spark = 0\n  count_apache = 0\n  for i in iterator:\n     if i =='spark':\n        count_spark = count_spark + 1\n     if i == 'apache':\n        count_apache = count_apache + 1\n  return (count_spark,count_apache)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3979754e-7800-42e8-afec-ba4a609d3de8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.mapPartitions(func).glom().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8317f4af-a0a1-4fb3-b7db-0877be721d21"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[143]: [[49, 39], [20, 13]]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[143]: [[49, 39], [20, 13]]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.mapPartitions(func).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e424962e-fad9-453a-9a02-b249c0a2439c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[144]: [49, 39, 20, 13]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[144]: [49, 39, 20, 13]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Math / Statistical Transformation\n# Transformation: sample\n# Q6: What if I want to work with samples instead of full data ?\n# Soltion: “sample” transformation helps us in taking samples instead of working on full data. The sample method will return a new RDD, containing a statistical sample of the original RDD.\n# We can pass the arguments insights as the sample operation:\n\n# “withReplacement = True” or False (to choose the sample with or without replacement)\n# “fraction = x” ( x= .4 means we want to choose 40% of data in “rdd” ) and “seed” for reproduce the results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d9077dd-4efb-451d-90c9-32c86b7c399c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_sampled = rdd3.sample(False, 0.4, 42)\nprint(len(rdd3.collect()),len(rdd3_sampled.collect()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1d69d46-3c18-40d7-8cbb-76fa87b96646"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">4768 1872\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">4768 1872\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Set Theory / Relational Transformation\n# Transformation: union\n# Q 7: What if I want to create a RDD which contains all the elements (a.k.a. union) of two RDDs ?\n# Solution: To do so, we can use “union” transformation on two RDDs. In Spark “union” transformation will return a new RDD by taking the union of two RDDs. Please note that duplicate items will not be removed in the new RDD. To illustrate this:\n\n# I am first going to create a two sample RDD ( say sample1, sample2 ) from the “rdd3” by taking 20% sample for each.\n# Apply a union transformation on sample1, sample2."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"042bd2ad-5640-4b6d-a537-e0e4e79ee069"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["sample1 = rdd3.sample(False,0.2,42)\nsample2 =rdd3.sample(False,0.2,42)\nunion_of_sample1_sample2 = sample1.union(sample2)\nprint(len(sample1.collect()), len(sample2.collect()),len(union_of_sample1_sample2.collect()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92b24c22-3a1d-4abc-8579-fa76d0a78c10"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">931 931 1862\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">931 931 1862\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# From the above output, we can see that the “sample1”, “sample2” both have 914 elements each. And in the “union_of_sample1_sample2”, we have 1828 elements which shows that union operation didn’t remove the duplicate elements."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ae0bd73-7c50-4023-980f-87a96c048dfb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: join\n# Q 8: If we want to join the two pair RDDs based on their key.\n# Solution: The “join” transformation can help us join two pairs of RDDs based on their key. To show that:\n\n# First create the two sample (key,value) pair RDDs (“sample1”, “sample2”) from the “rdd3_mapped” same as I did for “union” transformation\n#  Apply a “join” transformation on “sample1”,  “sample2”."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a191481c-75ce-4474-9f7f-f81f0e71132b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["sample1 = rdd3_mapped.sample(False,.2,42)\nsample2 = rdd3_mapped.sample(False,.2,42)\njoin_on_sample1_sample2 = sample1.join(sample2)\njoin_on_sample1_sample2.take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"203caa06-a8f1-4894-8fe8-466559fcdc52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[151]: [(&#39;think&#39;, (1, 1)), (&#39;even&#39;, (1, 1))]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[151]: [(&#39;think&#39;, (1, 1)), (&#39;even&#39;, (1, 1))]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: distinct\n# Q 9: How to calculate distinct elements in a RDD ?\n# Solution: We can apply “distinct” transformation on RDD to get the distinct elements. Let’s see how many distinct words do we have in the “rdd3”."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77ef7a39-834c-42d1-9896-232d47c4dcc4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_distinct = rdd3.distinct()\nlen(rdd3_distinct.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"582777d5-0184-4220-a7a5-eea0f08a8ee0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[153]: 1485</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[153]: 1485</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Data Structure / I/O Transformation\n# Transformation: coalesce\n# Q 10: What if I want to reduce the number of partition of a RDD and get the result in a new RDD?\n# Solution: We will use “coalesce” transformation here. To demonstrate that:\n\n# Let’s first check the number of partition in rdd3."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9263b92b-193a-40fc-a5bc-04f1dee3132c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25bdc829-9fe1-4044-b4dc-e9c4c2371b90"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[155]: 2</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[155]: 2</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 2. And now apply coalesce transformation on “rdd3” , get the results in “rdd3_coalesce” and see the number of partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0289db39-660e-4767-95aa-5d71a016c4fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_coalesce = rdd3.coalesce(1)\nrdd3_coalesce.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55692a3e-07cc-4619-86ce-2997ef88ac16"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[157]: 1</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[157]: 1</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# In some previous examples of transformation I already used some of the actions on different RDDs for printing the result. For example,”take” to print the first n elements of a RDD , “getNumPartitions” to know how many partition a RDD has and “collect” to print all elements of RDD.\n\n# Now, I will take few more actions to demonstrate how we can get the results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c4acc87-3933-424b-9efe-ea9772fde5fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# General Actions\n# Action: getNumPartitions\n# Q 11: How do I find out number of parition in RDD ?\n\n# Solution: With “getNumPartitions”, we can find out that how many partitions exist in our RDD. Let’s see how many partition our initial RDD (\"rdd3\") has."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2254955b-6127-4377-bbc5-7d363e0092d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a199a997-f9c5-4409-86e7-bf6754d01086"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[160]: 2</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[160]: 2</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Action: Reduce\n# Q 12: If I want to find out the sum the all numbers in a RDD.\n\n# Solution: To demonstrate this, I will:\n\n# First create a RDD from a list of number from (1,1000) called “num_rdd”.\n# Use a reduce action and pass a function through it (lambda x,y:  x+y).\n# A reduce action is use for aggregating all the elements of RDD by applying pairwise user function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa4378c-f01b-4d00-961b-edc385ce2122"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["num_rdd = sc.parallelize(range(1,1000))\nnum_rdd.reduce(lambda x,y: x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7dca145-ca49-4a00-b719-3031d9b85b3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[162]: 499500</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[162]: 499500</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# In the code above, I first created a RDD(“num_rdd”) from the list and then I applied a reduce action on it to sum all  the numbers in “num_rdd”."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"473cdec7-7757-42c7-900b-3b61c4b717c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Mathematical / Statistical Actions\n# Action: count\n# Q 13: Count the number of elements in RDD.\n\n# Solution: The count action will count the number of elements in RDD. To see that, let’s apply count action on “rdd3” to count the number of words in \"rdd3\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92aa13ef-3d59-4258-b6d9-829a98977b28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdecd0e7-c709-44ec-a63a-9c3736427ff9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[165]: 4768</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[165]: 4768</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Action: max, min, sum, variance and stdev\n# To take the maximum, minimum, sum, variance and standard deviation of a RDD, we can apply “max”, “min”, “sum”, “variance” and “stdev” actions. Let’s take the maximum, minimum, sum, variance and standard deviation of “num_rdd”."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c78005d2-7593-41d1-b193-ff302b55a5ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["num_rdd.max(),num_rdd.min(), num_rdd.sum(),num_rdd.variance(),num_rdd.stdev()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"837cf568-8858-4280-ba8b-55f5234e2963"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[167]: (999, 1, 499500, 83166.66666666667, 288.38631497813253)</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[167]: (999, 1, 499500, 83166.66666666667, 288.38631497813253)</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bce4a11-37d7-437d-8802-c0c0e0e8d485"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82ab66e2-4706-4305-bbc3-5dc9c7ebb9d3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1103ce9-fddb-486e-a0ab-23118aeebb89"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84da5d60-7a48-4cd4-9712-77131f8183ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60fb6112-da22-4cb7-9fff-6ed155a4b046"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark test","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4287172876670188}},"nbformat":4,"nbformat_minor":0}
