{"cells":[{"cell_type":"code","source":["# Creating RDDs \n# RDDs can be created from in-memory collections, or from data stored in external data sources (e.g., files on HDFS).  \n# The example below creates an RDD from an in-memory collection:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":true,"inputWidgets":{},"nuid":"c32b1ca5-6480-4f58-95e9-aea1b68e97cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# RDD(Resilient Distributed Dataset)\nwordsRDD=sc.parallelize([\"fish\",\"cat\",\"dog\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25f668bc-5f4a-44aa-9279-9e1b06dc1a75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["wordsRDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2424b569-cc22-4312-81e3-2619e9cbbfb4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[173]: ParallelCollectionRDD[230] at readRDDFromInputStream at PythonRDD.scala:413</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[173]: ParallelCollectionRDD[230] at readRDDFromInputStream at PythonRDD.scala:413</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Provide an instance of an iterable object (sequence)\n# as argument. Note that the Spark Context is automatically \n# instantiated by the notebook, and is bound to the 'sc' variable.\n# It will need to be created explicitly for a standalone program.\nrdd = sc.parallelize(range(30))\nrdd"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79ec3f19-4712-462a-92ef-40df032cc98c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[175]: PythonRDD[234] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[175]: PythonRDD[234] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Spark Context can also be accessed through the Spark Session object, which is\n# automatically created and bound to the variable 'spark'\nrdd1 = spark.sparkContext.parallelize(range(3, 5))\nrdd1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8531faf2-554f-41c0-b1a5-6b3cc9599943"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[176]: PythonRDD[236] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[176]: PythonRDD[236] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Collect\n# An RDD does not live in the local memory of a driver program (unless in local mode), but rather is broken into pieces and distributed across a cluster. While there, it undergoes a series of transformations as prescribed by the driver program (which is converted into a lineage graph, or DAG). The transformations are not actually executed until an action is called. An action causes data to be produced and materialized in either the driver program memory, or an external store (e.g., HDFS).\n\n# The collect() action below causes the RDD content to be shipped back to a driver program and materialized in its memory as a list. Do not call it unless you are sure your RDD is sufficiently small!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e80749ac-3435-4829-9323-5627870ff0f4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["result = rdd.collect()\nprint(type(result))\nprint(result)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01df9b41-1a00-456d-867a-94f861ba32be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["result = rdd1.collect()\nprint(type(result))\nprint(result)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"397179d5-65c4-42a9-94b3-8f8b050c3373"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n[3, 4]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n[3, 4]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Map\n# Map transformation is one of the most commonly used. As in Python, it applies a given function to every element of an RDD, and returns the resulting RDD. Note that this new RDD is not actual data! It is just an abstract handle encapsulating the transformation outcome that we can use to invoke another transformation or action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb0f256c-6217-4178-a9f2-8585b1c7e488"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(rdd.map(lambda x: x*x).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b8c2826-38a5-4e3d-b42f-e3a36a8b9df6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reduce\n# Reduce is a commonly used action. It takes an RDD as input and returns a single reduced value. As in Python, to obtain the result, it repeatedly applies a reduce operator to the elements of an RDD. The reduce operator is a function that takes two elements as input and returns one as output.\n\n# Workers execute reduce in parallel: Each executor reduces the data local to it, and the results from all executors are combined. It is therefore extermely important that the reduce operator is both commutative and associative as the execution order cannot be known in advance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccab8886-8902-4b1f-9509-afa2cedd3aa3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Reduce rdd to the sum of its elements\nrdd.reduce(lambda x,y:x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0cae1ed-ab3a-43c5-b73e-fdb5eb9c3c37"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[183]: 435</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[183]: 435</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Here is a slightly more elaborate example. Try to figure out what this code is doing, and the result is going to be. Then run the code to check yourself."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1904f8c-6ac1-4b52-b645-1fb3797d3190"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["words=['this','is','the','best','linux','ever','Saikiran']\nwordRDD=sc.parallelize(words)\nwordRDD.reduce(lambda w,v: w if len(w)>len(v) else v)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0b8f3b5a-dbe9-4aff-b1f6-7f5cff4c3756"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[186]: &#39;Saikiran&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[186]: &#39;Saikiran&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Simple Map and Reduce pipelines\n# Compute the sum of squares. Sequential syntax:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c4d5dc0-48f2-47b1-88ca-79be4a8f7e74"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["Squares=rdd.map(lambda x:x*x)\nSquares.reduce(lambda x,y:x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"259257d5-197c-4be6-aecf-54070814575f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[210]: 7500000</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[210]: 7500000</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#cascading is more compact and \"functional\": \nrdd.map(lambda x:x*x).reduce(lambda x,y:x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27a7358d-1c3b-4f19-8113-be97b17ffe3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[212]: 7500000</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[212]: 7500000</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Extracting basic information about an RDD\n# RDD's typically have hundreds of thousands of elements. It usually makes no sense to print out the content of a whole RDD. Here are some ways to get manageable amounts of information about an RDD.\n\n# Create an RDD of length n which is a repetition of the pattern 1,2,3,4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70f94284-29ce-497e-9fcb-fe8bb38f3c47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["n = 1000000\nrdd = sc.parallelize([1,2,3,4]*int(n/4))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42393ab7-d564-47ee-8478-6d12b33797a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use the count() action to find the number of elements in the RDD\nrdd.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7218809-4e5e-4a30-81e0-2e5d1abfb16f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[215]: 1000000</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[215]: 1000000</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Get the first few elements of an RDD\n# Both first() and take() are actions\nprint('first element=', rdd.first())\nprint('first 5 elements = ', rdd.take(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"169fc337-9548-40f7-93f5-2bc6ee77d2d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">first element= 1\nfirst 5 elements =  [1, 2, 3, 4, 1, 2, 3, 4, 1, 2]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">first element= 1\nfirst 5 elements =  [1, 2, 3, 4, 1, 2, 3, 4, 1, 2]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Sampling an RDD\n# RDDs are often very large.\n# Aggregates, such as averages, can be approximated efficiently by using a sample.\n# Sampling is done in parallel and requires limited computation.\n\n\n# The transformation sample(withReplacement, p) generates a sample of the elements of the RDD. where\n\n# withReplacement is a boolean flag indicating whether or not a an element in the RDD can be sampled more than once.\n# p is the probability of accepting each element into the sample. Note that as the sampling is performed independently in each partition, the number of elements in the sample changes from sample to sample."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c85af7f-f9eb-4d68-a64c-5c31523659d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# get a sample whose expected size is m\n# Note that the size of the sample is different in different runs\nm=5.\nprint('sample1=', rdd.sample(False, m/n).collect()) \nprint('sample2=', rdd.sample(False, m/n).collect())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"372960b4-f292-4d36-95fe-b9f5272fc619"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">sample1= [4, 1, 2, 1, 4, 4]\nsample2= [1, 4, 4, 3]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sample1= [4, 1, 2, 1, 4, 4]\nsample2= [1, 4, 4, 3]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Filtering an RDD\n# The transformation filter(func) returns a new RDD formed by selecting those elements of the source RDD on which func returns true"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9d30baf-d597-40ad-9622-734898021598"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print('the number of elements in rdd that are > 3 =', rdd.filter(lambda n: n > 3).count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a90707b5-321a-46b9-b383-3d8e41206fea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">the number of elements in rdd that are &gt; 3 = 250000\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">the number of elements in rdd that are &gt; 3 = 250000\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Removing duplicate elements from an RDD\n# The transformation distinct() returns a new RDD that contains the distinct elements of the source RDD.\n\n# This operation requires a shuffle in order to detect duplication across partitions (we'll discuss the implications of this later in the class)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ff4535c-c146-4729-89d4-53fddf5c1894"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Remove duplicate element in DuplicateRDD, we get distinct RDD\nDuplicateRDD = sc.parallelize([1, 1, 2, 2, 3, 3])\nprint('DuplicateRDD=', DuplicateRDD.collect())\nprint('DistinctRDD = ', DuplicateRDD.distinct().collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3785db26-0fea-430d-8358-b5dd8cd60d01"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">DuplicateRDD= [1, 1, 2, 2, 3, 3]\nDistinctRDD =  [1, 2, 3]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DuplicateRDD= [1, 1, 2, 2, 3, 3]\nDistinctRDD =  [1, 2, 3]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Flat Map\n# The transformation flatMap(func) is similar to map, but the result is \"flattened\" before being returned."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e66dbd3-f564-4feb-b619-430771ca8877"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["text=[\"you are my sunshine\",\"my only sunshine\"]\ntext_file = sc.parallelize(text)\n\n# map each line in text to a list of words\nprint('map:',text_file.map(lambda line: line.split(\" \")).collect())\n\n# create a single list of words by combining the words from all of the lines\nprint('flatmap:',text_file.flatMap(lambda line: line.split(\" \")).collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c761ae8b-57d9-4e16-a2c5-a6847b171e68"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">map: [[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\nflatmap: [&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;, &#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">map: [[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\nflatmap: [&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;, &#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Exercise 1\n# Write a Spark program that uses the map() transformation followed by the collect() action to compute and output the list of cubes of integer numbers in a given range."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ba1c1ab-a6a3-44b9-9294-0da239e872f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# A sample range of numbers. Experiment with different ones.\nr = range(5)\n\n# Create an RDD from r\nnumsRDD = sc.parallelize(r)\n\n# Use map to conver it to an RDD consisting of a sequence of cubes followed by collect() to materialize the result\n# as a list\nnumsRDD.map(lambda x: x*x*x).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca5c05e8-c4d5-487a-ba99-729f655729a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[36]: [0, 1, 8, 27, 64]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: [0, 1, 8, 27, 64]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 2\n# Write a Spark program that uses filter() followed by collect() to output the list of all objects which are positive integers in the given list of objects."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b79979b5-222e-4338-9430-3fdf514b78ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Sample input list of objects\nlst = [1234, '666', 'hi, there!', -23, 0, 'bye', 1]\n# Your code goes here\nlstRDD = sc.parallelize(lst)\nlstRDD.filter(lambda x: type(x) is int and x > 0).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bcd9836-5149-4a81-a9bd-1691449f583e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[38]: [1234, 1]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: [1234, 1]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 3\n# Combine the filter() transformation from the previous exercise with the map() transformation to obtain the list of cubes of all positive numbers found in an input list of objects."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ee569bd-5449-451c-aee8-95000b124e72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Sample input list of objects\nlst = [1234, '666', 'hi, there!', -23, 0, 'bye', 1]\n# Your code goes here\nlstRDD.filter(lambda x: type(x) is int and x > 0).map(lambda x : x*x*x).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39d36d05-5b83-4d31-aa34-aa205c559074"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[40]: [1879080904, 1]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: [1879080904, 1]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 4\n# Given an in-memory collection of words, write a Spark program that computes and outputs the longest one.\n# A sample collection. Feel free to modify as you wish.\nwords=['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine','Saikiranlearnspython']\n# Your code goes here\nsc.parallelize(words).reduce(lambda p, w: p if len(p) > len(w) else w)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3628f0c-4891-49fb-a1e8-5d5c4b76c389"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[49]: &#39;Saikiranlearnspython&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[49]: &#39;Saikiranlearnspython&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 5\n# Given an in-memory collection of strings, write a Spark program that computes the average of their length. Hint: One possible implementation will first map every word to its length, reduce it to the sum, and then divide it by the length of the list. Note that for larger datasets, it is better to use the count() action to count the number of elements."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5fca340-94df-498c-af78-220334472821"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# A sample collection. Feel free to modify as you wish.\nwords=['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine']\n# Your code goes here\nwordsRDD = sc.parallelize(words)\nsum = wordsRDD.map(lambda x: len(x)).reduce(lambda acc, y: acc+y)\nprint(sum)\ncnt = wordsRDD.count()\navg = sum / cnt\nprint('Avg = {0:.1f}'.format(avg))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47262381-1e7c-4af3-be9c-25658ae30344"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">30\nAvg = 4.3\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">30\nAvg = 4.3\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 6\n# Modify the above program to only include the words longer than 2 characters. Hint: Use filter transformation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe93f420-cfec-48fd-9ce0-9c82e749b9f2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# A sample collection. Feel free to modify as you wish.\nwords=['you', 'are', 'my', 'sunshine', 'my', 'only', 'sunshine']\n# Your code goes here\nlensRDD = sc.parallelize(words).map(lambda x: len(x)).filter(lambda x: x > 2)\nsum = lensRDD.reduce(lambda acc, y: acc+y)\ncnt = lensRDD.count()\navg = sum / cnt\nprint('Avg = {0:.1f}'.format(avg))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c672b33-c14d-429d-be67-efbc8d12a042"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Avg = 5.2\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Avg = 5.2\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Exercise 7\n# Repeat Exercise 4 but assume the input is a list of sentences, and not individual words. Hint: Use the flatmap() transformation discussed above to flatten the list of words returned by split(), and then apply reduce() as in Exercise 4. Do the same for Exercises 5 and 6."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f529276-72ca-45ef-8a38-01fd185b80b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\n# A sample collection. Feel free to modify as you wish.\ntext=[\"you are my sunshine\",\"my only sunshine\"]\n# Your code goes here\n\nwordsRDD = sc.parallelize(text).map(lambda s: s.split())\nwordsRDD.flatMap(lambda s: s).reduce(lambda p, w: p if len(p) > len(w) else w)\nprint(wordsRDD.collect())\n\n# toDebugString is useful to print out the lineage graph for debugging.\nprint(wordsRDD.toDebugString().decode('utf-8'))\n\n# sometimes useful to print out help info\nhelp(pyspark.RDD.flatMap)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3968e91-842b-4b27-a52e-113a7e7196ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\n(8) PythonRDD[100] at collect at &lt;command-1710337403652345&gt;:8 []\n |  ParallelCollectionRDD[98] at readRDDFromInputStream at PythonRDD.scala:413 []\nHelp on function flatMap in module pyspark.rdd:\n\nflatMap(self, f, preservesPartitioning=False)\n    Return a new RDD by first applying a function to all elements of this\n    RDD, and then flattening the results.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])\n    &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n    [1, 1, 1, 2, 2, 3]\n    &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n    [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[[&#39;you&#39;, &#39;are&#39;, &#39;my&#39;, &#39;sunshine&#39;], [&#39;my&#39;, &#39;only&#39;, &#39;sunshine&#39;]]\n(8) PythonRDD[100] at collect at &lt;command-1710337403652345&gt;:8 []\n  ParallelCollectionRDD[98] at readRDDFromInputStream at PythonRDD.scala:413 []\nHelp on function flatMap in module pyspark.rdd:\n\nflatMap(self, f, preservesPartitioning=False)\n    Return a new RDD by first applying a function to all elements of this\n    RDD, and then flattening the results.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])\n    &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n    [1, 1, 1, 2, 2, 3]\n    &gt;&gt;&gt; sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n    [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["input_data = [\"Python Pool\",\n        \"Latracal Solutions\",\n        \"Python pool is best\",\n        \"Basic command in python\"]\nrdd=spark.sparkContext.parallelize(input_data)\nrdd2=rdd.flatMap(lambda x: x.split(\" \"))\nlist(rdd2.collect())\n#    print(ele)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f314087d-2046-456b-8da9-679ba10ca43f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[88]: [&#39;Python&#39;,\n &#39;Pool&#39;,\n &#39;Latracal&#39;,\n &#39;Solutions&#39;,\n &#39;Python&#39;,\n &#39;pool&#39;,\n &#39;is&#39;,\n &#39;best&#39;,\n &#39;Basic&#39;,\n &#39;command&#39;,\n &#39;in&#39;,\n &#39;python&#39;,\n &#39;sa&#39;,\n &#39;i&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[88]: [&#39;Python&#39;,\n &#39;Pool&#39;,\n &#39;Latracal&#39;,\n &#39;Solutions&#39;,\n &#39;Python&#39;,\n &#39;pool&#39;,\n &#39;is&#39;,\n &#39;best&#39;,\n &#39;Basic&#39;,\n &#39;command&#39;,\n &#39;in&#39;,\n &#39;python&#39;,\n &#39;sa&#39;,\n &#39;i&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["text=[\"you are my sunshine\",\"my only sunshine\"]\nrdd = sc.parallelize(text)\nrdd.flatMap(lambda x: x.split())\\\n     .map(lambda x: len(x))\\\n     .map(lambda x: (x, 1))\\\n     .reduceByKey(lambda x, y: x + y)\\\n     .collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"783c241a-d085-4a8d-bbf7-94654dbae1d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[93]: [(8, 2), (2, 2), (3, 2), (4, 1)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[93]: [(8, 2), (2, 2), (3, 2), (4, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["text=[\"you are my sunshine\",\"my only sunshine\"]\nrdd = sc.parallelize(text)\nrdd.flatMap(lambda x: x.split()).map(lambda x: len(x)).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d79207b-bf49-4075-bf4f-e8f9cd6bd955"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[97]: [(8, 2), (2, 2), (3, 2), (4, 1)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[97]: [(8, 2), (2, 2), (3, 2), (4, 1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21f36cdc-ff13-4df9-b77e-6b2c44bf2124"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#flat file storage \nrdd = sc.textFile(\"dbfs:/FileStore/shared_uploads/csaikiran482@gmail.com/blogtexts\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f16cc9b-14dd-449b-9761-74637bc4b3dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b2c6e22-3c8b-44b5-b148-aed482553486"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[78]: [&#39;Think of it for a moment  1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.&#39;,\n &#39;&#39;,\n &#39;Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.&#39;,\n &#39;&#39;,\n &#39;On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[78]: [&#39;Think of it for a moment  1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.&#39;,\n &#39;&#39;,\n &#39;Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.&#39;,\n &#39;&#39;,\n &#39;On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def Func(lines):\n      lines = lines.lower()\n      lines = lines.split()\n      return lines\nrdd1 = rdd.map(Func)\nrdd1.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80ed2c04-cef9-4789-91ef-f3771f838295"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[79]: [[&#39;think&#39;,\n  &#39;of&#39;,\n  &#39;it&#39;,\n  &#39;for&#39;,\n  &#39;a&#39;,\n  &#39;moment&#39;,\n  &#39;&#39;,\n  &#39;1&#39;,\n  &#39;qunitillion&#39;,\n  &#39;=&#39;,\n  &#39;1&#39;,\n  &#39;million&#39;,\n  &#39;billion!&#39;,\n  &#39;can&#39;,\n  &#39;you&#39;,\n  &#39;imagine&#39;,\n  &#39;how&#39;,\n  &#39;many&#39;,\n  &#39;drives&#39;,\n  &#39;/&#39;,\n  &#39;cds&#39;,\n  &#39;/&#39;,\n  &#39;blue-ray&#39;,\n  &#39;dvds&#39;,\n  &#39;would&#39;,\n  &#39;be&#39;,\n  &#39;required&#39;,\n  &#39;to&#39;,\n  &#39;store&#39;,\n  &#39;them?&#39;,\n  &#39;it&#39;,\n  &#39;is&#39;,\n  &#39;difficult&#39;,\n  &#39;to&#39;,\n  &#39;imagine&#39;,\n  &#39;this&#39;,\n  &#39;scale&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation&#39;,\n  &#39;even&#39;,\n  &#39;as&#39;,\n  &#39;a&#39;,\n  &#39;data&#39;,\n  &#39;science&#39;,\n  &#39;professional.&#39;,\n  &#39;while&#39;,\n  &#39;this&#39;,\n  &#39;pace&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation&#39;,\n  &#39;is&#39;,\n  &#39;very&#39;,\n  &#39;exciting,&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;created&#39;,\n  &#39;entirely&#39;,\n  &#39;new&#39;,\n  &#39;set&#39;,\n  &#39;of&#39;,\n  &#39;challenges&#39;,\n  &#39;and&#39;,\n  &#39;has&#39;,\n  &#39;forced&#39;,\n  &#39;us&#39;,\n  &#39;to&#39;,\n  &#39;find&#39;,\n  &#39;new&#39;,\n  &#39;ways&#39;,\n  &#39;to&#39;,\n  &#39;handle&#39;,\n  &#39;big&#39;,\n  &#39;huge&#39;,\n  &#39;data&#39;,\n  &#39;effectively.&#39;],\n [],\n [&#39;big&#39;,\n  &#39;data&#39;,\n  &#39;is&#39;,\n  &#39;not&#39;,\n  &#39;a&#39;,\n  &#39;new&#39;,\n  &#39;phenomena.&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;been&#39;,\n  &#39;around&#39;,\n  &#39;for&#39;,\n  &#39;a&#39;,\n  &#39;while&#39;,\n  &#39;now.&#39;,\n  &#39;however,&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;become&#39;,\n  &#39;really&#39;,\n  &#39;important&#39;,\n  &#39;with&#39;,\n  &#39;this&#39;,\n  &#39;pace&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation.&#39;,\n  &#39;in&#39;,\n  &#39;past,&#39;,\n  &#39;several&#39;,\n  &#39;systems&#39;,\n  &#39;were&#39;,\n  &#39;developed&#39;,\n  &#39;for&#39;,\n  &#39;processing&#39;,\n  &#39;big&#39;,\n  &#39;data.&#39;,\n  &#39;most&#39;,\n  &#39;of&#39;,\n  &#39;them&#39;,\n  &#39;were&#39;,\n  &#39;based&#39;,\n  &#39;on&#39;,\n  &#39;mapreduce&#39;,\n  &#39;framework.&#39;,\n  &#39;these&#39;,\n  &#39;frameworks&#39;,\n  &#39;typically&#39;,\n  &#39;rely&#39;,\n  &#39;on&#39;,\n  &#39;use&#39;,\n  &#39;of&#39;,\n  &#39;hard&#39;,\n  &#39;disk&#39;,\n  &#39;for&#39;,\n  &#39;saving&#39;,\n  &#39;and&#39;,\n  &#39;retrieving&#39;,\n  &#39;the&#39;,\n  &#39;results.&#39;,\n  &#39;however,&#39;,\n  &#39;this&#39;,\n  &#39;turns&#39;,\n  &#39;out&#39;,\n  &#39;to&#39;,\n  &#39;be&#39;,\n  &#39;very&#39;,\n  &#39;costly&#39;,\n  &#39;in&#39;,\n  &#39;terms&#39;,\n  &#39;of&#39;,\n  &#39;time&#39;,\n  &#39;and&#39;,\n  &#39;speed.&#39;],\n [],\n [&#39;on&#39;,\n  &#39;the&#39;,\n  &#39;other&#39;,\n  &#39;hand,&#39;,\n  &#39;organizations&#39;,\n  &#39;have&#39;,\n  &#39;never&#39;,\n  &#39;been&#39;,\n  &#39;more&#39;,\n  &#39;hungrier&#39;,\n  &#39;to&#39;,\n  &#39;add&#39;,\n  &#39;a&#39;,\n  &#39;competitive&#39;,\n  &#39;differentiation&#39;,\n  &#39;through&#39;,\n  &#39;understanding&#39;,\n  &#39;this&#39;,\n  &#39;data&#39;,\n  &#39;and&#39;,\n  &#39;offering&#39;,\n  &#39;its&#39;,\n  &#39;customer&#39;,\n  &#39;a&#39;,\n  &#39;much&#39;,\n  &#39;better&#39;,\n  &#39;experience.&#39;,\n  &#39;imagine&#39;,\n  &#39;how&#39;,\n  &#39;valuable&#39;,\n  &#39;would&#39;,\n  &#39;be&#39;,\n  &#39;facebook,&#39;,\n  &#39;if&#39;,\n  &#39;it&#39;,\n  &#39;did&#39;,\n  &#39;not&#39;,\n  &#39;understand&#39;,\n  &#39;your&#39;,\n  &#39;interests&#39;,\n  &#39;well?&#39;,\n  &#39;the&#39;,\n  &#39;traditional&#39;,\n  &#39;hard&#39;,\n  &#39;disk&#39;,\n  &#39;based&#39;,\n  &#39;mapreduce&#39;,\n  &#39;kind&#39;,\n  &#39;of&#39;,\n  &#39;frameworks&#39;,\n  &#39;do&#39;,\n  &#39;not&#39;,\n  &#39;help&#39;,\n  &#39;much&#39;,\n  &#39;to&#39;,\n  &#39;address&#39;,\n  &#39;this&#39;,\n  &#39;challenge.&#39;]]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[79]: [[&#39;think&#39;,\n  &#39;of&#39;,\n  &#39;it&#39;,\n  &#39;for&#39;,\n  &#39;a&#39;,\n  &#39;moment&#39;,\n  &#39;&#39;,\n  &#39;1&#39;,\n  &#39;qunitillion&#39;,\n  &#39;=&#39;,\n  &#39;1&#39;,\n  &#39;million&#39;,\n  &#39;billion!&#39;,\n  &#39;can&#39;,\n  &#39;you&#39;,\n  &#39;imagine&#39;,\n  &#39;how&#39;,\n  &#39;many&#39;,\n  &#39;drives&#39;,\n  &#39;/&#39;,\n  &#39;cds&#39;,\n  &#39;/&#39;,\n  &#39;blue-ray&#39;,\n  &#39;dvds&#39;,\n  &#39;would&#39;,\n  &#39;be&#39;,\n  &#39;required&#39;,\n  &#39;to&#39;,\n  &#39;store&#39;,\n  &#39;them?&#39;,\n  &#39;it&#39;,\n  &#39;is&#39;,\n  &#39;difficult&#39;,\n  &#39;to&#39;,\n  &#39;imagine&#39;,\n  &#39;this&#39;,\n  &#39;scale&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation&#39;,\n  &#39;even&#39;,\n  &#39;as&#39;,\n  &#39;a&#39;,\n  &#39;data&#39;,\n  &#39;science&#39;,\n  &#39;professional.&#39;,\n  &#39;while&#39;,\n  &#39;this&#39;,\n  &#39;pace&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation&#39;,\n  &#39;is&#39;,\n  &#39;very&#39;,\n  &#39;exciting,&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;created&#39;,\n  &#39;entirely&#39;,\n  &#39;new&#39;,\n  &#39;set&#39;,\n  &#39;of&#39;,\n  &#39;challenges&#39;,\n  &#39;and&#39;,\n  &#39;has&#39;,\n  &#39;forced&#39;,\n  &#39;us&#39;,\n  &#39;to&#39;,\n  &#39;find&#39;,\n  &#39;new&#39;,\n  &#39;ways&#39;,\n  &#39;to&#39;,\n  &#39;handle&#39;,\n  &#39;big&#39;,\n  &#39;huge&#39;,\n  &#39;data&#39;,\n  &#39;effectively.&#39;],\n [],\n [&#39;big&#39;,\n  &#39;data&#39;,\n  &#39;is&#39;,\n  &#39;not&#39;,\n  &#39;a&#39;,\n  &#39;new&#39;,\n  &#39;phenomena.&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;been&#39;,\n  &#39;around&#39;,\n  &#39;for&#39;,\n  &#39;a&#39;,\n  &#39;while&#39;,\n  &#39;now.&#39;,\n  &#39;however,&#39;,\n  &#39;it&#39;,\n  &#39;has&#39;,\n  &#39;become&#39;,\n  &#39;really&#39;,\n  &#39;important&#39;,\n  &#39;with&#39;,\n  &#39;this&#39;,\n  &#39;pace&#39;,\n  &#39;of&#39;,\n  &#39;data&#39;,\n  &#39;generation.&#39;,\n  &#39;in&#39;,\n  &#39;past,&#39;,\n  &#39;several&#39;,\n  &#39;systems&#39;,\n  &#39;were&#39;,\n  &#39;developed&#39;,\n  &#39;for&#39;,\n  &#39;processing&#39;,\n  &#39;big&#39;,\n  &#39;data.&#39;,\n  &#39;most&#39;,\n  &#39;of&#39;,\n  &#39;them&#39;,\n  &#39;were&#39;,\n  &#39;based&#39;,\n  &#39;on&#39;,\n  &#39;mapreduce&#39;,\n  &#39;framework.&#39;,\n  &#39;these&#39;,\n  &#39;frameworks&#39;,\n  &#39;typically&#39;,\n  &#39;rely&#39;,\n  &#39;on&#39;,\n  &#39;use&#39;,\n  &#39;of&#39;,\n  &#39;hard&#39;,\n  &#39;disk&#39;,\n  &#39;for&#39;,\n  &#39;saving&#39;,\n  &#39;and&#39;,\n  &#39;retrieving&#39;,\n  &#39;the&#39;,\n  &#39;results.&#39;,\n  &#39;however,&#39;,\n  &#39;this&#39;,\n  &#39;turns&#39;,\n  &#39;out&#39;,\n  &#39;to&#39;,\n  &#39;be&#39;,\n  &#39;very&#39;,\n  &#39;costly&#39;,\n  &#39;in&#39;,\n  &#39;terms&#39;,\n  &#39;of&#39;,\n  &#39;time&#39;,\n  &#39;and&#39;,\n  &#39;speed.&#39;],\n [],\n [&#39;on&#39;,\n  &#39;the&#39;,\n  &#39;other&#39;,\n  &#39;hand,&#39;,\n  &#39;organizations&#39;,\n  &#39;have&#39;,\n  &#39;never&#39;,\n  &#39;been&#39;,\n  &#39;more&#39;,\n  &#39;hungrier&#39;,\n  &#39;to&#39;,\n  &#39;add&#39;,\n  &#39;a&#39;,\n  &#39;competitive&#39;,\n  &#39;differentiation&#39;,\n  &#39;through&#39;,\n  &#39;understanding&#39;,\n  &#39;this&#39;,\n  &#39;data&#39;,\n  &#39;and&#39;,\n  &#39;offering&#39;,\n  &#39;its&#39;,\n  &#39;customer&#39;,\n  &#39;a&#39;,\n  &#39;much&#39;,\n  &#39;better&#39;,\n  &#39;experience.&#39;,\n  &#39;imagine&#39;,\n  &#39;how&#39;,\n  &#39;valuable&#39;,\n  &#39;would&#39;,\n  &#39;be&#39;,\n  &#39;facebook,&#39;,\n  &#39;if&#39;,\n  &#39;it&#39;,\n  &#39;did&#39;,\n  &#39;not&#39;,\n  &#39;understand&#39;,\n  &#39;your&#39;,\n  &#39;interests&#39;,\n  &#39;well?&#39;,\n  &#39;the&#39;,\n  &#39;traditional&#39;,\n  &#39;hard&#39;,\n  &#39;disk&#39;,\n  &#39;based&#39;,\n  &#39;mapreduce&#39;,\n  &#39;kind&#39;,\n  &#39;of&#39;,\n  &#39;frameworks&#39;,\n  &#39;do&#39;,\n  &#39;not&#39;,\n  &#39;help&#39;,\n  &#39;much&#39;,\n  &#39;to&#39;,\n  &#39;address&#39;,\n  &#39;this&#39;,\n  &#39;challenge.&#39;]]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Output is too long so, I have just attached a snippet of it. We can also see that our output is not flat (its a nested list). So for getting the flat output, we need to apply a transformation which will flatten the output, The transformation flatMap will help here:\n\n# The flatMap transformation will return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. This is the main difference between the flatMap and map transformations. Lets apply a flatMap transformation on rdd , then take the result of this transformation in rdd2 and print the result after applying this transformation.\n\nrdd2 = rdd.flatMap(Func)\nrdd2.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55ad8b16-f585-4cdb-a5b8-b4aa36231fa8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[80]: [&#39;think&#39;, &#39;of&#39;, &#39;it&#39;, &#39;for&#39;, &#39;a&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[80]: [&#39;think&#39;, &#39;of&#39;, &#39;it&#39;, &#39;for&#39;, &#39;a&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: filter\n# Q2: Next, I want to remove the words, which are not necessary to analyze this text. We call these words as stop words; Stop words do not add much value in a text. For example, is, am, are and the are few examples of stop words.\n\n# Solution: To remove the stop words, we can use a filter transformation which will return a new RDD containing only the elements that satisfy given condition(s). Lets apply filter transformation on rdd2 and get words which are not stop words and get the result in rdd3. To do that:\n\n#  We need to define the list of stop words in a variable called stopwords ( Here, I am selecting only a few words in stop words list instead of all the words).\n#  Apply filter on rdd2 (Check if individual words of rdd2 are in the stopwords list or not ).\n# We can check first 10 elements of rdd3 by applying take action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86acb2f3-bd29-4855-a9d9-f7fb69d582be"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["stopwords = ['is','am','are','the','for','a']\nrdd3= rdd2.filter(lambda x: x not in stopwords)\nrdd3.take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6afe9a11-0f38-49a8-b511-7d6a2f27d191"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[82]: [&#39;think&#39;, &#39;of&#39;, &#39;it&#39;, &#39;moment&#39;, &#39;&#39;, &#39;1&#39;, &#39;qunitillion&#39;, &#39;=&#39;, &#39;1&#39;, &#39;million&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[82]: [&#39;think&#39;, &#39;of&#39;, &#39;it&#39;, &#39;moment&#39;, &#39;&#39;, &#39;1&#39;, &#39;qunitillion&#39;, &#39;=&#39;, &#39;1&#39;, &#39;million&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: groupBy\n# Q3: After getting the results into rdd3, we want to group the words in rdd3 based on which letters they start with. For example, suppose I want to group each word of rdd3 based on first 3 characters.\n\n# Solution: The groupBy  transformation will group the data in the original RDD. It creates a set of key value pairs, where the key is output of a user function, and the value is all items for which the function yields this key.\n\n# We have to pass a function (in this case, I am using a lambda function) inside the groupBy which will take the first 3 characters of each word in rdd3.\n# The key is the first 3 characters and value is all the words which start with these 3 characters.\n# After applying groupBy function, we store the transformed result in rdd4 (RDDs are immutable  remember!). To view rdd4, we can print first (key, value) elements in rdd4."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6057fc2f-0fdc-4430-ab6c-15459c2b1619"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd4 = rdd3.groupBy(lambda w: w[0:3])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a17e8a4-ef99-4984-adb6-ba241d06b941"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print([(k, list(k)) for (k,v) in rdd4.take(10)])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7edf6973-a6f7-4b6c-b84f-ea6464d5c9c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[(&#39;thi&#39;, [&#39;t&#39;, &#39;h&#39;, &#39;i&#39;]), (&#39;of&#39;, [&#39;o&#39;, &#39;f&#39;]), (&#39;1&#39;, [&#39;1&#39;]), (&#39;qun&#39;, [&#39;q&#39;, &#39;u&#39;, &#39;n&#39;]), (&#39;=&#39;, [&#39;=&#39;]), (&#39;mil&#39;, [&#39;m&#39;, &#39;i&#39;, &#39;l&#39;]), (&#39;bil&#39;, [&#39;b&#39;, &#39;i&#39;, &#39;l&#39;]), (&#39;ima&#39;, [&#39;i&#39;, &#39;m&#39;, &#39;a&#39;]), (&#39;cds&#39;, [&#39;c&#39;, &#39;d&#39;, &#39;s&#39;]), (&#39;dvd&#39;, [&#39;d&#39;, &#39;v&#39;, &#39;d&#39;])]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;thi&#39;, [&#39;t&#39;, &#39;h&#39;, &#39;i&#39;]), (&#39;of&#39;, [&#39;o&#39;, &#39;f&#39;]), (&#39;1&#39;, [&#39;1&#39;]), (&#39;qun&#39;, [&#39;q&#39;, &#39;u&#39;, &#39;n&#39;]), (&#39;=&#39;, [&#39;=&#39;]), (&#39;mil&#39;, [&#39;m&#39;, &#39;i&#39;, &#39;l&#39;]), (&#39;bil&#39;, [&#39;b&#39;, &#39;i&#39;, &#39;l&#39;]), (&#39;ima&#39;, [&#39;i&#39;, &#39;m&#39;, &#39;a&#39;]), (&#39;cds&#39;, [&#39;c&#39;, &#39;d&#39;, &#39;s&#39;]), (&#39;dvd&#39;, [&#39;d&#39;, &#39;v&#39;, &#39;d&#39;])]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: groupBy\n# Q3: After getting the results into rdd3, we want to group the words in rdd3 based on which letters they start with. For example, suppose I want to group each word of rdd3 based on first 3 characters.\n\n# Solution: The groupBy  transformation will group the data in the original RDD. It creates a set of key value pairs, where the key is output of a user function, and the value is all items for which the function yields this key.\n\n# We have to pass a function (in this case, I am using a lambda function) inside the groupBy which will take the first 3 characters of each word in rdd3.\n# The key is the first 3 characters and value is all the words which start with these 3 characters.\n# After applying groupBy function, we store the transformed result in rdd4 (RDDs are immutable  remember!). To view rdd4, we can print first (key, value) elements in rdd4."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ff3c344-b316-4a59-87a7-a2fca9041c04"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_mapped = rdd3.map(lambda x: (x,1))\nrdd3_grouped = rdd3_mapped.groupByKey()\nprint(list((j[0], list(j[1])) for j in rdd3_grouped.take(5)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e46c6bd9-caef-4f06-8754-afbb429237e3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[(&#39;think&#39;, [1, 1]), (&#39;of&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (&#39;1&#39;, [1, 1]), (&#39;qunitillion&#39;, [1]), (&#39;=&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;think&#39;, [1, 1]), (&#39;of&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (&#39;1&#39;, [1, 1]), (&#39;qunitillion&#39;, [1]), (&#39;=&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: groupByKey / reduceByKey \n# Q4: What if we want to calculate how many times each word is coming in corpus ?\n\n# Solution: We can apply the groupByKey / reduceByKey transformations on (key,val) pair RDD. The groupByKey will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values.\n\n# To use groupbyKey / reduceByKey transformation to find the frequencies of each words, you can follow the steps below:\n\n# A (key,val) pair RDD is required; In this (key,val) pair RDD, key is the word and val is 1 for each word in RDD (1 represents the number for the each word in rdd3).\n# To apply groupbyKey / reduceByKey on rdd3, we need to first convert rdd3 to (key,val) pair RDD.\n \n\n# Lets see, how to convert rdd3 to new mapped (key,val) RDD. And then we can apply groupbyKey / reduceByKey transformation on this RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86ff0113-786f-478a-832a-7b101b81679a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_mapped = rdd3.map(lambda x: (x,1))\nrdd3_grouped = rdd3_mapped.groupByKey()\nprint(list((j[0], list(j[1])) for j in rdd3_grouped.take(5)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d5cd53a-1b1d-4eac-a83c-b8d3aa498502"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[(&#39;think&#39;, [1, 1]), (&#39;of&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (&#39;1&#39;, [1, 1]), (&#39;qunitillion&#39;, [1]), (&#39;=&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;think&#39;, [1, 1]), (&#39;of&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), (&#39;1&#39;, [1, 1]), (&#39;qunitillion&#39;, [1]), (&#39;=&#39;, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# After seeing the result of the above code, I rechecked the corpus to know, how many times the word manager is there, so I found that manager is written more then once. I figure out that there are more words like manager. , manager, and manager:. Lets filter manager, in rdd3."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c169578-a79c-466d-a0fe-42078c8d20c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.filter(lambda x: x == 'manager,').collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a2aad74-b3f2-4ff8-9d74-082fef46c1fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[134]: [&#39;manager,&#39;, &#39;manager,&#39;, &#39;manager,&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[134]: [&#39;manager,&#39;, &#39;manager,&#39;, &#39;manager,&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# We can see that in above output, we have multiple words with manager in our corpus. To overcome this situation we can do several things. We could apply a regular expression to remove unnecessary punctuation from the words. For the purpose of this article, I am skipping that part.\n\n# Until now we have not calculated the frequencies / counts of each words. Lets proceed further :"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49953805-a7dd-48bd-aa07-43ba9cf03c1b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_freq_of_words = rdd3_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91c5147d-6e31-40ab-a7e0-ee42b29abde8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# In the above code, I first applied mapValues transformation on rdd3_grouped. The mapValues (only applicable on pair RDD) transformation is like a map (can be applied on any RDD) transform but it has one difference that when we apply map transform on pair RDD we can access the key and value both of this RDD but in case of mapValues transformation, it will transform the values by applying some function and key will not be affected. So for example, in above code I applied sum, which will calculate the sum (counts) for the each word.\n\n# After applying mapValues  transformation I want to sort the words based on their frequencies so for doing that I am first converting a ( word, frequency ) pair to ( frequency,word ) so that our key and values will be interchanged then, I will apply a sorting based on key and then get a result in rdd3_freq_of_words. We can see that 10 most frequent words I used in my previous blog by applying take action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5c8c2e7-79b5-49a5-a4f9-2f6886f6d76c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_freq_of_words.take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e6d25c7-cd92-4d2b-8940-598cf8a47587"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[138]: [(164, &#39;to&#39;),\n (143, &#39;in&#39;),\n (122, &#39;of&#39;),\n (106, &#39;and&#39;),\n (103, &#39;we&#39;),\n (69, &#39;spark&#39;),\n (64, &#39;this&#39;),\n (63, &#39;data&#39;),\n (55, &#39;can&#39;),\n (52, &#39;apache&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[138]: [(164, &#39;to&#39;),\n (143, &#39;in&#39;),\n (122, &#39;of&#39;),\n (106, &#39;and&#39;),\n (103, &#39;we&#39;),\n (69, &#39;spark&#39;),\n (64, &#39;this&#39;),\n (63, &#39;data&#39;),\n (55, &#39;can&#39;),\n (52, &#39;apache&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Above output shows that I used words spark 69 times and Apache 52 times in my previous blog.\n\n \n\n# We can also use reduceByKey transformation for counting the frequencies of each word in (key,value) pair RDD. Lets see how will we do this."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f446d97-952e-435b-b21e-fa75fec7ba2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9476c67c-8657-41b1-a4f8-af6d09f9252d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[140]: [(164, &#39;to&#39;),\n (143, &#39;in&#39;),\n (122, &#39;of&#39;),\n (106, &#39;and&#39;),\n (103, &#39;we&#39;),\n (69, &#39;spark&#39;),\n (64, &#39;this&#39;),\n (63, &#39;data&#39;),\n (55, &#39;can&#39;),\n (52, &#39;apache&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[140]: [(164, &#39;to&#39;),\n (143, &#39;in&#39;),\n (122, &#39;of&#39;),\n (106, &#39;and&#39;),\n (103, &#39;we&#39;),\n (69, &#39;spark&#39;),\n (64, &#39;this&#39;),\n (63, &#39;data&#39;),\n (55, &#39;can&#39;),\n (52, &#39;apache&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: mapPartitions\n# Q5: How do I perform a task (say count the words spark and apache in rdd3) separatly on each partition and get the output of the task performed in these partition ?\n# Soltion: We can do this by applying mapPartitions transformation. The mapPartitions is like a map transformation but runs separately on different partitions of a RDD. So, for counting the frequencies of words spark and apache in each partition of RDD, you can follow the steps:\n\n# Create a function called func which will count the frequencies for these words\n#  Then, pass the function defined in step1 to the mapPartitions transformation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18784636-3f41-45c8-ae55-b85125a0988e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def func(iterator):\n  count_spark = 0\n  count_apache = 0\n  for i in iterator:\n     if i =='spark':\n        count_spark = count_spark + 1\n     if i == 'apache':\n        count_apache = count_apache + 1\n  return (count_spark,count_apache)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3979754e-7800-42e8-afec-ba4a609d3de8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.mapPartitions(func).glom().collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8317f4af-a0a1-4fb3-b7db-0877be721d21"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[143]: [[49, 39], [20, 13]]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[143]: [[49, 39], [20, 13]]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.mapPartitions(func).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e424962e-fad9-453a-9a02-b249c0a2439c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[144]: [49, 39, 20, 13]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[144]: [49, 39, 20, 13]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Math / Statistical Transformation\n# Transformation: sample\n# Q6: What if I want to work with samples instead of full data ?\n# Soltion: sample transformation helps us in taking samples instead of working on full data. The sample method will return a new RDD, containing a statistical sample of the original RDD.\n# We can pass the arguments insights as the sample operation:\n\n# withReplacement = True or False (to choose the sample with or without replacement)\n# fraction = x ( x= .4 means we want to choose 40% of data in rdd ) and seed for reproduce the results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d9077dd-4efb-451d-90c9-32c86b7c399c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_sampled = rdd3.sample(False, 0.4, 42)\nprint(len(rdd3.collect()),len(rdd3_sampled.collect()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1d69d46-3c18-40d7-8cbb-76fa87b96646"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">4768 1872\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">4768 1872\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Set Theory / Relational Transformation\n# Transformation: union\n# Q 7: What if I want to create a RDD which contains all the elements (a.k.a. union) of two RDDs ?\n# Solution: To do so, we can use union transformation on two RDDs. In Spark union transformation will return a new RDD by taking the union of two RDDs. Please note that duplicate items will not be removed in the new RDD. To illustrate this:\n\n# I am first going to create a two sample RDD ( say sample1, sample2 ) from the rdd3 by taking 20% sample for each.\n# Apply a union transformation on sample1, sample2."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"042bd2ad-5640-4b6d-a537-e0e4e79ee069"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["sample1 = rdd3.sample(False,0.2,42)\nsample2 =rdd3.sample(False,0.2,42)\nunion_of_sample1_sample2 = sample1.union(sample2)\nprint(len(sample1.collect()), len(sample2.collect()),len(union_of_sample1_sample2.collect()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92b24c22-3a1d-4abc-8579-fa76d0a78c10"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">931 931 1862\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">931 931 1862\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# From the above output, we can see that the sample1, sample2 both have 914 elements each. And in the union_of_sample1_sample2, we have 1828 elements which shows that union operation didnt remove the duplicate elements."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ae0bd73-7c50-4023-980f-87a96c048dfb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: join\n# Q 8: If we want to join the two pair RDDs based on their key.\n# Solution: The join transformation can help us join two pairs of RDDs based on their key. To show that:\n\n# First create the two sample (key,value) pair RDDs (sample1, sample2) from the rdd3_mapped same as I did for union transformation\n#  Apply a join transformation on sample1,  sample2."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a191481c-75ce-4474-9f7f-f81f0e71132b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["sample1 = rdd3_mapped.sample(False,.2,42)\nsample2 = rdd3_mapped.sample(False,.2,42)\njoin_on_sample1_sample2 = sample1.join(sample2)\njoin_on_sample1_sample2.take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"203caa06-a8f1-4894-8fe8-466559fcdc52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[151]: [(&#39;think&#39;, (1, 1)), (&#39;even&#39;, (1, 1))]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[151]: [(&#39;think&#39;, (1, 1)), (&#39;even&#39;, (1, 1))]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Transformation: distinct\n# Q 9: How to calculate distinct elements in a RDD ?\n# Solution: We can apply distinct transformation on RDD to get the distinct elements. Lets see how many distinct words do we have in the rdd3."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77ef7a39-834c-42d1-9896-232d47c4dcc4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_distinct = rdd3.distinct()\nlen(rdd3_distinct.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"582777d5-0184-4220-a7a5-eea0f08a8ee0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[153]: 1485</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[153]: 1485</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Data Structure / I/O Transformation\n# Transformation: coalesce\n# Q 10: What if I want to reduce the number of partition of a RDD and get the result in a new RDD?\n# Solution: We will use coalesce transformation here. To demonstrate that:\n\n# Lets first check the number of partition in rdd3."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9263b92b-193a-40fc-a5bc-04f1dee3132c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25bdc829-9fe1-4044-b4dc-e9c4c2371b90"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[155]: 2</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[155]: 2</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 2. And now apply coalesce transformation on rdd3 , get the results in rdd3_coalesce and see the number of partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0289db39-660e-4767-95aa-5d71a016c4fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3_coalesce = rdd3.coalesce(1)\nrdd3_coalesce.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55692a3e-07cc-4619-86ce-2997ef88ac16"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[157]: 1</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[157]: 1</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# In some previous examples of transformation I already used some of the actions on different RDDs for printing the result. For example,take to print the first n elements of a RDD , getNumPartitions to know how many partition a RDD has and collect to print all elements of RDD.\n\n# Now, I will take few more actions to demonstrate how we can get the results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c4acc87-3933-424b-9efe-ea9772fde5fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# General Actions\n# Action: getNumPartitions\n# Q 11: How do I find out number of parition in RDD ?\n\n# Solution: With getNumPartitions, we can find out that how many partitions exist in our RDD. Lets see how many partition our initial RDD (\"rdd3\") has."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2254955b-6127-4377-bbc5-7d363e0092d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a199a997-f9c5-4409-86e7-bf6754d01086"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[160]: 2</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[160]: 2</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Action: Reduce\n# Q 12: If I want to find out the sum the all numbers in a RDD.\n\n# Solution: To demonstrate this, I will:\n\n# First create a RDD from a list of number from (1,1000) called num_rdd.\n# Use a reduce action and pass a function through it (lambda x,y:  x+y).\n# A reduce action is use for aggregating all the elements of RDD by applying pairwise user function."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa4378c-f01b-4d00-961b-edc385ce2122"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["num_rdd = sc.parallelize(range(1,1000))\nnum_rdd.reduce(lambda x,y: x+y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7dca145-ca49-4a00-b719-3031d9b85b3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[162]: 499500</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[162]: 499500</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# In the code above, I first created a RDD(num_rdd) from the list and then I applied a reduce action on it to sum all  the numbers in num_rdd."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"473cdec7-7757-42c7-900b-3b61c4b717c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Mathematical / Statistical Actions\n# Action: count\n# Q 13: Count the number of elements in RDD.\n\n# Solution: The count action will count the number of elements in RDD. To see that, lets apply count action on rdd3 to count the number of words in \"rdd3\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92aa13ef-3d59-4258-b6d9-829a98977b28"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["rdd3.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdecd0e7-c709-44ec-a63a-9c3736427ff9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[165]: 4768</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[165]: 4768</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Action: max, min, sum, variance and stdev\n# To take the maximum, minimum, sum, variance and standard deviation of a RDD, we can apply max, min, sum, variance and stdev actions. Lets take the maximum, minimum, sum, variance and standard deviation of num_rdd."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c78005d2-7593-41d1-b193-ff302b55a5ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["num_rdd.max(),num_rdd.min(), num_rdd.sum(),num_rdd.variance(),num_rdd.stdev()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"837cf568-8858-4280-ba8b-55f5234e2963"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[167]: (999, 1, 499500, 83166.66666666667, 288.38631497813253)</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[167]: (999, 1, 499500, 83166.66666666667, 288.38631497813253)</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bce4a11-37d7-437d-8802-c0c0e0e8d485"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82ab66e2-4706-4305-bbc3-5dc9c7ebb9d3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1103ce9-fddb-486e-a0ab-23118aeebb89"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84da5d60-7a48-4cd4-9712-77131f8183ae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60fb6112-da22-4cb7-9fff-6ed155a4b046"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Spark test","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4287172876670188}},"nbformat":4,"nbformat_minor":0}
